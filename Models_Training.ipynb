{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d485bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d516a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "import shap\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_test(model, X_test, y_test):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Predict Test Data \n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]>0.5:\n",
    "            y_pred[i]=1\n",
    "        else:\n",
    "            y_pred[i]=0\n",
    "    \n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score, and kappa score\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate area under curve (AUC)\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    #MCC\n",
    "    mcc=matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # Display confussion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    total=sum(sum(cm))\n",
    "    \n",
    "    #accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    spec = cm[0,0]/(cm[0,1]+cm[0,0])\n",
    "    sen= cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'mcc':mcc,\n",
    "            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm, 'sen': sen, 'spec':spec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_train(model, X_train, y_train):\n",
    "    from sklearn import metrics\n",
    "    conf_matrix_list_of_arrays = []\n",
    "    mcc_array=[]\n",
    "    #cv = KFold(n_splits=5)\n",
    "    #cv = StratifiedKFold(n_splits=5)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1)\n",
    "    lst_accu = []\n",
    "    AUC_list=[]\n",
    "    prec_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='precision'))\n",
    "    recall_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='recall'))\n",
    "    f1_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='f1'))\n",
    "    Acc=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy'))\n",
    "    print(Acc)\n",
    "    for train_index, test_index in cv.split(X_train, y_train): \n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index] \n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index] \n",
    "        model.fit(X_train_fold, y_train_fold) \n",
    "        lst_accu.append(model.score(X_test_fold, y_test_fold))\n",
    "        acc=np.mean(lst_accu)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_fold, model.predict(X_test_fold))\n",
    "        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "        cm = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
    "        mcc_array.append(matthews_corrcoef(y_test_fold, model.predict(X_test_fold)))\n",
    "        mcc=np.mean(mcc_array, axis=0)\n",
    "        \n",
    "        AUC=metrics.roc_auc_score( y_test_fold, model.predict_proba(X_test_fold)[:,1])\n",
    "        AUC_list.append(AUC)\n",
    "        auc=np.mean(AUC_list)\n",
    "        \n",
    "        \n",
    "    total=sum(sum(cm))\n",
    "    accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
    "    sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "       \n",
    "    \n",
    "    return {'prec_train': prec_train, 'recall_train': recall_train, 'f1_train': f1_train, 'cm': cm, 'mcc': mcc,'Acc':Acc,\n",
    "           'sen':sensitivity,'spec':specificity, 'acc':acc, 'lst_accu':lst_accu, 'AUC':auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817870a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature descriptors of training data\n",
    "df1 = pd.read_csv(\" /path......\")\n",
    "df2 = pd.read_csv(\" /path......\")\n",
    ".\n",
    ".\n",
    ".\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.concat([df1, ....],axis = 1)\n",
    "X_train = data1.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ac248",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "lab=len(X_train)/2\n",
    "pos_labels = np.ones(int(lab))\n",
    "neg_labels = np.zeros(int(lab))\n",
    "y_train = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef873a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only = X_train\n",
    "labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102efe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature descriptors of independent data\n",
    "df1 = pd.read_csv(\" /path......\")\n",
    "df2 = pd.read_csv(\" /path......\")\n",
    ".\n",
    ".\n",
    ".\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Independent data\n",
    "data2=pd.concat([df1, ....],axis = 1)\n",
    "X_test_ind = data2.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ae2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_ind=np.array(X_test_ind)\n",
    "lab=len(X_test_ind)/2\n",
    "pos_labels = np.ones(int(lab))\n",
    "neg_labels = np.zeros(int(lab))\n",
    "y_test_ind = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba0841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56f57038",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RF_objective(trial):\n",
    "    \n",
    "    \n",
    "    folds=10\n",
    "    scores = []\n",
    "    test_accs = []\n",
    "   \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=72)\n",
    "    \n",
    "    # kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_only,labels)):\n",
    "        \n",
    "        \n",
    "    \n",
    "        X_train0, X_test1 = data_only[train_index], data_only[test_index]\n",
    "        y_train0, y_test1 = labels[train_index], labels[test_index]\n",
    "    \n",
    "        X_train1, X_validation, y_train1, y_validation = train_test_split(X_train0, y_train0, test_size=0.01, random_state=72, shuffle=True)\n",
    "        \n",
    "       \n",
    "    \n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 2, 1000)\n",
    "    criterion =  trial.suggest_categorical('criterion', ['entropy','gini' ])\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 80)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 40)\n",
    "    min_samples_split= trial.suggest_int(\"min_samples_split\", 2, 40)\n",
    "    min_samples_leaf= trial.suggest_int('min_samples_leaf', 2, 40)\n",
    "    max_features= trial.suggest_categorical('max_features', ['auto','sqrt','log2'])\n",
    "   \n",
    "    ## Create Model\n",
    "    rf_model = RandomForestClassifier(max_depth = max_depth, min_samples_split=min_samples_split, \n",
    "                                      min_samples_leaf = min_samples_leaf, n_estimators = n_estimators,\n",
    "                                   max_features=max_features, random_state=72)\n",
    "\n",
    "\n",
    "    rf_model.fit(X_train1,  y_train1)    \n",
    "    y_pred = rf_model.predict(X_test1)\n",
    "    accuracy = accuracy_score(y_test1, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "RF_study = optuna.create_study(direction='maximize')\n",
    "RF_study.optimize(RF_objective, n_trials=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RF_study.best_trial)  # Show the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_RF=RandomForestClassifier(**RF_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_RF, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['Acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Acc value from CM is: \", train_eval['acc'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "#print('5 accuracies: ', train_eval['lst_accu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "#rfc.fit(X_train, y_train)\n",
    "dtc_eval = evaluate_model_test(optimized_RF, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66402b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb91ee1",
   "metadata": {},
   "source": [
    "# ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c238b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    folds=10\n",
    "    scores = []\n",
    "    test_accs = []\n",
    "   \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=72)\n",
    "    \n",
    "    # kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_only,labels)):\n",
    "        X_train0, X_test1 = data_only[train_index], data_only[test_index]\n",
    "        y_train0, y_test1 = labels[train_index], labels[test_index]\n",
    "    \n",
    "        X_train1, X_validation, y_train1, y_validation = train_test_split(X_train0, y_train0, test_size=0.001, random_state=72, shuffle=True)\n",
    "        \n",
    "       \n",
    "    ### define params grid to search maximum accuracy\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000, step=5)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 80, step=1)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 6, 30)\n",
    "    criterion = trial.suggest_categorical('criterion', ['log_loss','gini', 'entropy'])\n",
    "    max_features = trial.suggest_categorical('max_features',  ['auto','sqrt','log2'])\n",
    "    min_samples_split= trial.suggest_int(' min_samples_split', 2,  30, step=1)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20, step=1)\n",
    "    \n",
    "    #splitter = trial.suggest_categorical('splitter', ['best', 'random'])\n",
    "\n",
    "    ### modeling with suggested params\n",
    "    etc_model = ExtraTreesClassifier(n_estimators = n_estimators,\n",
    "                                 max_depth = max_depth,\n",
    "                                 max_leaf_nodes = max_leaf_nodes,\n",
    "                                 criterion = criterion,\n",
    "                                 max_features = max_features,\n",
    "                                 min_samples_split = min_samples_split,\n",
    "                                 min_samples_leaf = min_samples_leaf,\n",
    "                                 #splitter= splitter,\n",
    "                                 random_state = 42)\n",
    "    ### fit\n",
    "    etc_model.fit(X_train1, y_train1) # train on train data\n",
    "    score = cross_val_score(etc_model,X_train1, y_train1, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "#     y_pred = etc_model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     return accuracy\n",
    "    \n",
    "    \n",
    "study = optuna.create_study(direction='maximize') # maximize accuracy\n",
    "study.optimize(objective, n_trials=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbe554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(etc_study.best_trial.params)  # Show the best value.\n",
    "optimized_etc =ExtraTreesClassifier(**etc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5015d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_etc, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['Acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51385a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_etc, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d40f8",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "   \n",
    "    folds=10\n",
    "    scores = []\n",
    "    test_accs = []\n",
    "   \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    \n",
    "    # kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_only,labels)):\n",
    "        X_train0, X_test1 = data_only[train_index], data_only[test_index]\n",
    "        y_train0, y_test1 = labels[train_index], labels[test_index]\n",
    "    \n",
    "        X_train1, X_validation, y_train1, y_validation = train_test_split(X_train0, y_train0, test_size=0.2, random_state=92, shuffle=True)\n",
    "        \n",
    "       \n",
    "  \n",
    "        cat_model = CatBoostClassifier(\n",
    "        iterations=trial.suggest_int(\"iterations\", 1, 100),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
    "        depth=trial.suggest_int(\"depth\", 4, 16),\n",
    "        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1e-8, 100.0, log=True),\n",
    "        bootstrap_type=trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\"]),\n",
    "        random_strength=trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        bagging_temperature=trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "        od_type=trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "        od_wait=trial.suggest_int(\"od_wait\", 10, 80),\n",
    "        verbose=False\n",
    "    )\n",
    "        \n",
    "        \n",
    "        model2 = cat_model.fit(X_train1, y_train1)\n",
    "        \n",
    "        \n",
    "        y_pred = model2.predict(X_test1)\n",
    "        \n",
    "        test_acc = metrics.accuracy_score(y_test1, y_pred)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "        print('fold',i)\n",
    "      \n",
    "        # print(test_accs)\n",
    "        i=i+1\n",
    "    return np.mean(test_accs)\n",
    "        \n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=10000000000)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f89ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study_cbc.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study_cbc.best_trial.params)  # Show the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "optimized_cbt = catboost.CatBoostClassifier(\"best_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_cbc, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_cbc, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2f661",
   "metadata": {},
   "source": [
    "# XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e719712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as XGB\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcee452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    folds=10\n",
    "    scores = []\n",
    "    test_accs = []\n",
    "   \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    \n",
    "    # kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_only,labels)):\n",
    "        \n",
    "        \n",
    "    \n",
    "        X_train0, X_test1 = data_only[train_index], data_only[test_index]\n",
    "        y_train0, y_test1 = labels[train_index], labels[test_index]\n",
    "    \n",
    "        X_train1, X_validation, y_train1, y_validation = train_test_split(X_train0, y_train0, test_size=0.01, random_state=92, shuffle=True)\n",
    "     \n",
    "      \n",
    "    params = {\n",
    "           'max_depth': trial.suggest_int('max_depth', 1, 80),\n",
    "           'learning_rate': trial.suggest_float('learning_rate', 0.01, 10.0),\n",
    "           'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "           'min_child_weight': trial.suggest_int('min_child_weight', 1, 40),\n",
    "           'gamma': trial.suggest_float('gamma', 1e-8, 10.0),\n",
    "           'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "           'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "           'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "           'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n",
    "           'eval_metric': 'mlogloss',\n",
    "           'use_label_encoder': False\n",
    "    }\n",
    "    optuna_model = XGBClassifier(**params)\n",
    "    optuna_model.fit(X_train1, y_train1)\n",
    "    y_pred = optuna_model.predict(X_test1)\n",
    "    accuracy = accuracy_score(y_test1, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=700)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54389ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XGB_study.best_trial)  # Show the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_xgb = XGBClassifier(**XGB_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_XGB, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e86f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_XGB, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429446a",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_wine\n",
    "import optuna\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    folds=10\n",
    "    scores = []\n",
    "    test_accs = []\n",
    "   \n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    \n",
    "    # kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=92)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_only,labels)):\n",
    "        \n",
    "        \n",
    "    \n",
    "        X_train0, X_test1 = data_only[train_index], data_only[test_index]\n",
    "        y_train0, y_test1 = labels[train_index], labels[test_index]\n",
    "    \n",
    "        X_train1, X_validation, y_train1, y_validation = train_test_split(X_train0, y_train0, test_size=0.01, random_state=92, shuffle=True)\n",
    "      \n",
    "    \n",
    "       \n",
    "    \n",
    "    param = {\n",
    "        \n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 40, step=1, log=False), \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 80, step=1, log=False), \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1, log=True), \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=10), \n",
    "        #'objective': 'multiclass', \n",
    "        #'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 40, log=False), \n",
    "        'subsample': trial.suggest_uniform('subsample', 0.2, 1.0), \n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.2, 1.0),\n",
    "        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 92\n",
    "         }\n",
    "    gbm = lgb.LGBMClassifier(**param)    \n",
    "    gbm.fit(X_train1, y_train1)\n",
    "    y_pred = gbm.predict(X_test1)\n",
    "    accuracy = accuracy_score(y_test1, y_pred)\n",
    "    return accuracy\n",
    "    \n",
    "sampler = TPESampler(seed=1)\n",
    "study = optuna.create_study(study_name=\"lightgbm\", direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lgbm_study.best_trial)  # Show the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3390c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best value:', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0525a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_gbm = lgb.LGBMClassifier(\" best_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4020d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_lgbm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_lgbm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b4535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26cc6f10",
   "metadata": {},
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP= MLPClassifier(solver='adam',activation='logistic', alpha=1e-2, hidden_layer_sizes=(20), random_state=1)\n",
    "MLP.fit(X_train, y_train)\n",
    "scores = cross_val_score(MLP, X_train, y_train, cv=cv,  scoring='accuracy')\n",
    "y_pred =MLP.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "print(scores.mean())\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe81926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(MLP, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fa37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "\n",
    "dtc_eval = evaluate_model_test(MLP, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
