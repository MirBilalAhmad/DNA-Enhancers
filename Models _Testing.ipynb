{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007966e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_train(model, X_train, y_train):\n",
    "    from sklearn import metrics\n",
    "    conf_matrix_list_of_arrays = []\n",
    "    mcc_array=[]\n",
    "    #cv = KFold(n_splits=5)\n",
    "    #cv = StratifiedKFold(n_splits=5)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)\n",
    "    lst_accu = []\n",
    "    AUC_list=[]\n",
    "    prec_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='precision'))\n",
    "    recall_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='recall'))\n",
    "    f1_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='f1'))\n",
    "    Acc=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy'))\n",
    "    print(Acc)\n",
    "    for train_index, test_index in cv.split(X_train, y_train): \n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index] \n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index] \n",
    "        model.fit(X_train_fold, y_train_fold) \n",
    "        lst_accu.append(model.score(X_test_fold, y_test_fold))\n",
    "        acc=np.mean(lst_accu)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_fold, model.predict(X_test_fold))\n",
    "        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "        cm = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
    "        mcc_array.append(matthews_corrcoef(y_test_fold, model.predict(X_test_fold)))\n",
    "        mcc=np.mean(mcc_array, axis=0)\n",
    "        \n",
    "        AUC=metrics.roc_auc_score( y_test_fold, model.predict_proba(X_test_fold)[:,1])\n",
    "        AUC_list.append(AUC)\n",
    "        auc=np.mean(AUC_list)\n",
    "        \n",
    "        \n",
    "    total=sum(sum(cm))\n",
    "    accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    specificity = cm[0,0]/(cm[0,1]+cm[0,0])\n",
    "    sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "       \n",
    "    \n",
    "    return {'prec_train': prec_train, 'recall_train': recall_train, 'f1_train': f1_train, 'cm': cm, 'mcc': mcc,'Acc':Acc,\n",
    "           'sen':sensitivity,'spec':specificity, 'acc':acc, 'lst_accu':lst_accu, 'AUC':auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de835912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_test(model, X_test, y_test):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Predict Test Data \n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]>0.5:\n",
    "            y_pred[i]=1\n",
    "        else:\n",
    "            y_pred[i]=0\n",
    "    \n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score, and kappa score\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate area under curve (AUC)\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    #MCC\n",
    "    mcc=matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # Display confussion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    total=sum(sum(cm))\n",
    "    \n",
    "    #accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    spec = cm[0,0]/(cm[0,1]+cm[0,0])\n",
    "    sen= cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'mcc':mcc,\n",
    "            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm, 'sen': sen, 'spec':spec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature descriptors of training data\n",
    "df1 = pd.read_csv(\" /path......\")\n",
    "df2 = pd.read_csv(\" /path......\")\n",
    ".\n",
    ".\n",
    ".\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aafe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.concat([df1, ....],axis = 1)\n",
    "X_train = data1.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "lab=len(X_train)/2\n",
    "pos_labels = np.ones(int(lab))\n",
    "neg_labels = np.zeros(int(lab))\n",
    "y_train = np.concatenate((pos_labels,neg_labels),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature descriptors of independent data\n",
    "df1 = pd.read_csv(\" /path......\")\n",
    "df2 = pd.read_csv(\" /path......\")\n",
    ".\n",
    ".\n",
    ".\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Independent feature descriptors \n",
    "data2=pd.concat([df1, ....],axis = 1)\n",
    "X_test_ind = data2.iloc[:,1:].values\n",
    "X_test_ind=np.array(X_test_ind)\n",
    "lab=len(X_test_ind)/2\n",
    "pos_labels = np.ones(int(lab))\n",
    "neg_labels = np.zeros(int(lab))\n",
    "y_test_ind = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b05d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6aa322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b15cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1,random_state=92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1b94c",
   "metadata": {},
   "source": [
    "# LGBM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44815a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit the model\n",
    "    \n",
    "import os\n",
    "import lightgbm as lgbm\n",
    "\n",
    "lgbm_model = lgbm.LGBMClassifier(\" ** best_params\")\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "#score = cross_val_score(lgbm_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "\n",
    "\n",
    "y_pred = lgbm_model.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print('Mean_Accuracy is', accuracy_mean)\n",
    "print('Ind Accuracy is', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(lgbm_model, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "#optimized_lgbm.fit(X_train, y_train)\n",
    "dtc_eval = evaluate_model_test(lgbm_model, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c2364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d62a22e",
   "metadata": {},
   "source": [
    "# MLP Classifier Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP= MLPClassifier(\"** Best_params\")\n",
    "MLP.fit(X_train, y_train)\n",
    "#scores = cross_val_score(MLP, X_train, y_train, cv=cv,  scoring='accuracy')\n",
    "y_pred =MLP.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print(scores.mean())\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b797a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(MLP, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc38f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "\n",
    "dtc_eval = evaluate_model_test(MLP, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03329086",
   "metadata": {},
   "source": [
    "# ETC Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223514df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e848755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbf61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# Fit the model\n",
    "etc_model = ExtraTreesClassifier(\" ** Best_params ** \")\n",
    "\n",
    "etc_model.fit(X_train, y_train)\n",
    "#score = cross_val_score(etc_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "\n",
    "\n",
    "y_pred = etc_model.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print('Mean_Accuracy is', accuracy_mean)\n",
    "print('Ind Accuracy is', accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632969eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(etc_model, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['Acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46280620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(etc_model, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65483cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243628fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23ed943c",
   "metadata": {},
   "source": [
    "# XGB Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51f90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Fit the model\n",
    "xgb_model = XGBClassifier(\" ** Best_params** \" )\n",
    "xgb_model.fit(X_train, y_train) \n",
    "\n",
    "#score = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print('Mean_Accuracy is', accuracy_mean)\n",
    "print('Ind Accuracy is', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(xgb_model, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(xgb_model, X_test_ind, y_test_ind)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27658ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7457385c",
   "metadata": {},
   "source": [
    "# Random forest Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3499cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb2e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07285196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Rf_model = RandomForestClassifier(\" ** Best_Params\")\n",
    "Rf_model.fit(X_train,y_train) \n",
    "#score = cross_val_score(Rf_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "\n",
    "\n",
    "y_pred = Rf_model.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print('Mean_Accuracy is', accuracy_mean)\n",
    "print('Ind Accuracy is', accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b117f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(Rf_model, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['Acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Acc value from CM is: \", train_eval['acc'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Model on Testing data\n",
    "\n",
    "test_eval = evaluate_model_test(Rf_model, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bbc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b48974e8",
   "metadata": {},
   "source": [
    "# CatBoost Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b37a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1,random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Fit the model\n",
    "CB_model = catboost.CatBoostClassifier(\" ** Best_params\")\n",
    "CB_model.fit(X_train, y_train)\n",
    "#score = cross_val_score(CB_model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "\n",
    "\n",
    "y_pred = CB_model.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print('Mean_Accuracy is', accuracy_mean)\n",
    "print('Ind Accuracy is', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(CB_model, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "test_eval = evaluate_model_test(CB_model, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114af1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "734900d2",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66575c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1,random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test_ind.shape, y_test_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9523408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vclf2 = VotingClassifier(estimators=[ ('RF', Rf_model),  ('XGB', xgb_model),('LGBM', lgbm_model),('ETC', etc_model),\n",
    "                                     ('MLP', MLP),('Catboost', CB_model)], voting='soft')\n",
    "vclf2.fit(X_train, y_train)\n",
    "#score = cross_val_score(vclf2, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "\n",
    "\n",
    "y_pred = vclf2.predict(X_test_ind)\n",
    "accuracy = accuracy_score(y_test_ind, y_pred)\n",
    "\n",
    "#print('Mean_Accuracy is', accuracy_mean)\n",
    "print('Ind Accuracy is', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(vclf2, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5994df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(vclf2, X_test_ind, y_test_ind)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5aaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff97a421",
   "metadata": {},
   "source": [
    "# Meta-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2930c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test_ind.shape, y_test_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining meta-classifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import metrics\n",
    "clf_stack = StackingClassifier(classifiers =[ ( lgbm_model ), ( Rf_model), (MLP),(xgb_model), (etc_model)], \n",
    "                               meta_classifier = CB_model, use_probas = True, use_features_in_secondary = True)\n",
    "clf_stack.fit(X_train, y_train)\n",
    "#score = cross_val_score(clf_stack, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "y=clf_stack.predict(X_test_ind)\n",
    "score=accuracy_score(y,y_test_ind)\n",
    "\n",
    "#print(accuracy_mean)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(clf_stack, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82816ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(clf_stack, X_test_ind, y_test_ind)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d7b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4de91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d215a76",
   "metadata": {},
   "source": [
    "# Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=[  (Rf_model), ( xgb_model),(etc_model),(lgbm_model),(CB_model),(MLP)], voting='soft')#, weights=[0.2,0.1,0.3,0.3, 0.1])\n",
    "eclf.fit(X_train, y_train)\n",
    "#score = cross_val_score(eclf, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "#accuracy_mean = score.mean()\n",
    "y_predd=eclf.predict(X_test_ind)\n",
    "\n",
    "score=accuracy_score(y_predd,y_test_ind)\n",
    "\n",
    "#print(accuracy_mean)\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d0eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40134cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(eclf, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85df44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(eclf, X_test_ind, y_test_ind)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from mlens.ensemble import SuperLearner\n",
    "from mlens.metrics.metrics import rmse\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble model named as super learner from mlens package.\n",
    "\n",
    "model = SuperLearner(\n",
    "    folds=10, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    [\n",
    "        \n",
    "        xgb_model,\n",
    "        lgbm_model,  \n",
    "        Rf_model,\n",
    "        CB_model,\n",
    "        etc_model,\n",
    "        MLP,\n",
    "    \n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "model.add_meta(\n",
    "   CatBoostClassifier()\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test_ind)\n",
    "\n",
    "print('SuperLearner Train accuracy: ', accuracy_score(y_test_ind, preds))\n",
    "print('SuperLearner ind-accuracy: ', f1_score(y_test_ind, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef569fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdict = {\n",
    "    'RF': RandomForestClassifier(random_state=40),\n",
    "    'XGB': XGBClassifier(random_state=25),\n",
    "    'LGBM': lgbm.LGBMClassifier(random_state=72),\n",
    "    'CABT': CatBoostClassifier(random_state=90),\n",
    "    'OARF': RandomForestClassifier(\" **params\"),\n",
    "    'OAXGB': XGBClassifier( \"**params\"),\n",
    "    'OALGBM': lgbm.LGBMClassifier(\"**params\"),\n",
    "    'OCAT': CatBoostClassifier(\"**params\"),\n",
    "    \n",
    "    'OAET': ExtraTreesClassifier(\"**params\"),\n",
    "    'ET': ExtraTreesClassifier(random_state=32),\n",
    "    'OMLP': MLPClassifier(\"**params\"),\n",
    "    'MLP': MLPClassifier(random_state= 72),\n",
    "#     'GB': GradientBoostingClassifier(random_state=42),\n",
    "#     'RDG': RidgeClassifier(random_state=42),\n",
    "#     'PCP': Perceptron(random_state=42),\n",
    "#     'PAC': PassiveAggressiveClassifier(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e101cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d662752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    model_names = list()\n",
    "    models_list = [\n",
    "         'XGB', 'LGBM', 'RF','CABT','ET', 'MLP','OARF', \n",
    "        'OAXGB',  'OALGBM','OCAT', 'OARF',\n",
    "           'OAET', 'OMLP',\n",
    "    ]\n",
    "    \n",
    "    head_list = [\n",
    "        'RF', \n",
    "        'XGB', \n",
    "        'LGBM', \n",
    "        'CABT',     \n",
    "        'ET',\n",
    "        'MLP'\n",
    "        \n",
    "    ]\n",
    "       \n",
    "    \n",
    "    n_models = trial.suggest_int(\"n_models\", 2, 6)\n",
    "    for i in range(n_models):\n",
    "        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n",
    "        if model_item not in model_names:\n",
    "            model_names.append(model_item)\n",
    "    \n",
    "    folds = trial.suggest_int(\"folds\", 5, 10)\n",
    "    \n",
    "    model = SuperLearner(\n",
    "        folds=folds, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    models = [\n",
    "        mdict[item] for item in model_names\n",
    "    ]\n",
    "    model.add(models)\n",
    "    head = trial.suggest_categorical('head', head_list)\n",
    "    model.add_meta(\n",
    "        mdict[head]\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "        \n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    model = create_model(trial)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test_ind)\n",
    "    score = accuracy_score(y_test_ind, preds)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", \n",
    "  \n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_params\n",
    "\n",
    "head = params['head']\n",
    "folds = params['folds']\n",
    "del params['head'], params['n_models'], params['folds']\n",
    "result = list()\n",
    "for key, value in params.items():\n",
    "    if value not in result:\n",
    "        result.append(value)\n",
    "        \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63330a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SuperLearner(\n",
    "    folds=folds, \n",
    "    random_state=72\n",
    ")\n",
    "\n",
    "models = [\n",
    "    mdict[item] for item in result\n",
    "]\n",
    "model.add(models)\n",
    "model.add_meta(mdict[head])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test_ind)\n",
    "\n",
    "print('Optimized SuperLearner accuracy: ', accuracy_score(y_test_ind, preds))\n",
    "#print('Optimized SuperLearner f1-score: ', f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test_ind.shape, y_test_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = np.array([X_test_ind])\n",
    "#y_test = np.array([y_test_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(model, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(model, X_test_ind, y_test_ind)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eca4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
